<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML on 郝鸿涛：Hongtao Hao</title><link>https://hongtaoh.com/tags/ml/</link><description>Recent content in ML on 郝鸿涛：Hongtao Hao</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 26 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://hongtaoh.com/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding K-Nearest Neighbors Using MNIST Dataset</title><link>https://hongtaoh.com/en/2022/09/26/knn/</link><pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate><guid>https://hongtaoh.com/en/2022/09/26/knn/</guid><description>To me, K Nearest Neighbors (KNN) is very intuitive. Take classification tasks as an example, what KNN does is to find K training examples that are &amp;ldquo;closest&amp;rdquo; to the test example. These K training examples have K labels. We find the most common label and it is the predicted label for the test example in question.
How to quantify &amp;ldquo;closest&amp;rdquo;? Two simple methods are Euclidean distance or Manhattan distance.
I&amp;rsquo;ll take the MNIST dataset as an example.</description></item><item><title>Derivatives of 1/x and square root of x</title><link>https://hongtaoh.com/en/2022/08/29/two-derivatives/</link><pubDate>Mon, 29 Aug 2022 17:10:40 -0500</pubDate><guid>https://hongtaoh.com/en/2022/08/29/two-derivatives/</guid><description>1. $y = \frac{1}{x}$ The derivative of $y = \frac{1}{x}$ can be computed in the following way.
First, I highly recommend you to watch this clip , where 3blue1brown visualizes this function.
The key part of the proof is that Since A and B are both on the curve of $y = \frac{1}{x}$, the x coordinate times the y coordinate is 1. That is to say: $A_x \cdot A_y = 1$ and $B_x \cdot B_y = 1$.</description></item><item><title>Why Is Dot Product Defined and Calculated That Way</title><link>https://hongtaoh.com/en/2022/08/27/dot-product/</link><pubDate>Sat, 27 Aug 2022 11:34:49 -0500</pubDate><guid>https://hongtaoh.com/en/2022/08/27/dot-product/</guid><description>Suppose we have:
$$\vec{a} = \begin{bmatrix} a_x \\ a_y \end{bmatrix}$$
$$\vec{b} = \begin{bmatrix} b_x \\ b_y \end{bmatrix}$$
I am wondering why $\vec{a} \cdot \vec{b} = a_x b_x + a_y b_y$.
Let&amp;rsquo;s prove this by having a concrete example:
$$\vec{a} = \begin{bmatrix} 0 \\ 3 \end{bmatrix}$$
$$\vec{b} = \begin{bmatrix} 1 \\ 3 \end{bmatrix}$$
If you are familiar with linear algebra, you&amp;rsquo;ll know that
$$\vec{a}\cdot \vec{b} = ||\vec{a}||\cdot ||\vec{b}|| \cdot \cos \theta$$</description></item><item><title>Explaining and Implementing Hierarchical Clustering</title><link>https://hongtaoh.com/en/2022/08/24/hier_cluster/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>https://hongtaoh.com/en/2022/08/24/hier_cluster/</guid><description>In this post, I&amp;rsquo;ll explain what hierarchical clustering is and how to implement it with Python.
import pandas as pd import numpy as np import matplotlib.pyplot as plt np.random.seed(1234) N = 10 # number of points matrix = np.random.rand(N, 2) # N points in 2 dimensional space M = matrix * 10 # multiply ten so that the numbers are easier to understand. # Otherwise, all numbers are between 0 and 1 M array([[1.</description></item></channel></rss>
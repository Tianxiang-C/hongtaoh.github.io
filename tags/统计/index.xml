<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>统计 on 郝鸿涛：Hongtao Hao</title><link>https://hongtaoh.com/tags/%E7%BB%9F%E8%AE%A1/</link><description>Recent content in 统计 on 郝鸿涛：Hongtao Hao</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 09 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://hongtaoh.com/tags/%E7%BB%9F%E8%AE%A1/index.xml" rel="self" type="application/rss+xml"/><item><title>假设检验、Z 分数、置信区间</title><link>https://hongtaoh.com/cn/2021/04/09/stats-01/</link><pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate><guid>https://hongtaoh.com/cn/2021/04/09/stats-01/</guid><description>统计的一个核心问题是，我们想要知道母体 (population) 的情况，但是现实中，母体的情况是未知的，这时我们就需要通过随机抽样的方法，获取样本，通过样本来了解总体。
序章 我们现在来假设一下。
你在北京（A市）上学，有个女朋友，大四快毕业的时候，你考上了新疆（(B省）的选调生，不过还好只是呆两年，两年后可以再回A市工作。女朋友拿到了一家互联网公司的offer。那么，一个世纪难题是，离这么远，要不要分手。你们相互谈心，互相觉得，两年的时间还好，也不长，而且通信这么发达，都可以每天微信视频聊天，即使异地恋应该也没问题。于是你们俩决定不分手，坚持异地恋。
好，然后你给你女朋友提出了一个要求。你说你在新疆工作很忙，微信联系的话你觉得不太方便，而且你很传统，喜欢手写的信，你就对你女朋友说：“我走后，你要每周给我写一封情书”。女朋友答应了。
喜获 A 市市长职位，惊闻女友似已变心 你们按原计划进行。你每周都能如约收到一封情书，你很开心。
很快，两年就要过去了，领导看你很棒，把你推荐给了B省的领导，领导知道你想回北京，就把你推荐给了A市的市领导，最终决定让你回去做副市长。
家人觉得你这马上要走上人生巅峰了，可是婚姻大事还没解决，就让你赶紧成婚。你也思考，是啊，和女朋友这么多年了，也该考虑婚姻大事了。你每周都能收到一封信，只是让你不爽的是，你感觉女朋友的情书一次比一次短，最后一封只有四个字：我挺好的。
这几天正好一个朋友来看你，给你八卦说经常看到你女朋友和她同事一起跑步，去健身房。你怀疑你被绿了。
你可马上是副市长的人了，你考虑终身大事的时候可是得慎重些。
你看着手里的情书，蓝瘦，香菇，但又不想轻易相信女朋友已经变心。你突然想到前几天在 Nature 上看到的一篇文章。文章作者声称，他发现人类过去、现在和未来，两个人之间的情书字数符合正态分布，标准差是250 (\(\sigma\)=250）。他还发现，这两个人如果相爱的话，虽然每次情书字数有差别，但平均的话应该是 1500 字。多了少了都不行。这个作者还因为这个发现，获得了诺贝尔“爱情学奖”。
检验 因此，你现在确定的是，女朋友这两年间写的 104 封信的字数是符合正态分布的，标准差是 250。那么，你直接算出来这 104 封信每封信的平均字数，如果平均数正好等于 1500 字的话，说明女朋友还是爱你的。然而，很不凑巧的是，有几天 B 省太冷了，你晚上为了取暖就把几封信（随机地）扔到火堆里取暖（女朋友知道了直接就开除你了），还剩下 25 封情书。所以现在你没办法通过计算总体（104 封情书）的平均字数来断定你女朋友是不是还爱着你了。
你数了数，这25封信的字数分别为：
773 1227 1188 790 1413 1146 1202 776 1400 1240 937 902 1421 1291 1569 1368 939 1363 1331 1088 889 772 683 1374 1535 这里说一下，其实这才常见的。大多数情况下我们都不可能获得总体的全部，只能通过抽样来推断总体。
这时候，你突然想到了你了大二的时候学过统计（虽然学得啥都已经送给老师了）。
你开始琢磨了：
你知道你女朋友如果还爱你的话，这 104 封信应该符合 X \(\sim\) N (1500, 250)，也就是下面这个分布：</description></item><item><title>R aggregate 求不同组平均数，如何处理缺失值</title><link>https://hongtaoh.com/cn/2020/03/06/r-aggregate-function-na/</link><pubDate>Fri, 06 Mar 2020 00:00:00 +0000</pubDate><guid>https://hongtaoh.com/cn/2020/03/06/r-aggregate-function-na/</guid><description>今天碰到的一个问题是，用 aggregate 求不同组平均数的时候，缺失值不好处理。下面通过用 iris 这个经典数据说一下如果处理缺失值。
iris[sample(nrow(iris),5),] # 随即抽几行，大致浏览数据格式 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 136 7.7 3.0 6.1 2.3 virginica ## 85 5.4 3.0 4.5 1.5 versicolor ## 33 5.2 4.1 1.5 0.1 setosa ## 50 5.0 3.3 1.4 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa iris[c(1,5,15,30,45,90,145),4] &amp;lt;- NA #制造缺失值 aggregate(x=iris$Petal.Width, by=list(iris$Species), FUN=mean) ## Group.1 x ## 1 setosa NA ## 2 versicolor NA ## 3 virginica NA aggregate(x=iris$Petal.</description></item><item><title>贝叶斯统计之概率</title><link>https://hongtaoh.com/cn/2020/02/02/bayesian-stats-chapter4/</link><pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate><guid>https://hongtaoh.com/cn/2020/02/02/bayesian-stats-chapter4/</guid><description>上篇，也就是第二章，k老师主要是简单介绍了一下贝叶斯的方法。第三章是介绍了 r 语言的基本操作，这里不再介绍。从第四章开始，整本书渐渐进入主体，开始涉及贝叶斯的核心。
第四章开始讲概率 (probability)。概率是用来描述不确定性 (uncertainty) 的。假如我现在抛一枚硬币，掉下来之后有可能是正面朝上，也可能是背面朝上。每当我们讨论一种结果的可能性的时候，我们其实在头脑里想到了所有可能的结果，这些所有可能的结果叫做样本空间 (sample space)。
如果这个硬币质地良好，那么正面朝上的概率是 50%。如果质地粗糙，那么正面朝上的概率有可能大于50%，也可能小于50%。我们用希腊字母 θ 来表示正面朝上的概率。如果 $\theta = 50\%$ ，那说明硬币质地良好。
另一个层面，我们对于 $\theta$ 的值其实也是有预设的。如果这枚硬币是国家铸币厂生产的，那么 θ=50% 的概率会很高，如 $p(\theta = 0.5)＝0.99$, 如果这枚硬币是小作坊生产的，那么 $p(\theta = 0.5)$ 可能只有0.01.
回到样本空间这个概念。抛一枚硬币这个事件的样本空间有2种可能的结果：正面朝上、背面朝上。猜测硬币的质地时，样本空间有无数种可能的结果：从 $\theta ＝0.0$ 到 $\theta ＝1.0$ 之间所有的数字都有可能。
你可能像我一样对掷硬币已经不耐烦了，因为已经在太多教材中看过。但是k教授说到，掷硬币可以用来代表几乎所有二元性的事件：选a还是选b、药物有效还是无效、正确还是错误等等。所以我们还是要研究掷硬币。
当我们说，一个质地良好的硬币，正面朝上的概率是50%时，我们是说，将这个硬币抛掷无数次，有50%的次数是正面朝上的。也就是说，这个概率是指，从长远来看的相对频率。
有两种方法来获取这个从长远来看的相对频率：
模拟； 数学公式推导 第一种方法，模拟。我们用 R 语言来模拟。假设计算机预算 $\theta = 0.8$，也就是正面朝上的概率是 80％，但是我们不知道。我们只能通过观察计算机的掷币结果来推测。
下面我们在计算机上模拟抛掷一枚 $\theta = 0.8$ 的硬币一万次。代码1如下：
n &amp;lt;- 10000 pheads &amp;lt;- 0.8 flips&amp;lt;-sample (x=c(0,1),prob=c(1-pheads, pheads),size=n, replace=TRUE) num.heads &amp;lt;- cumsum(flips) total.toss &amp;lt;- c(1:n) prob.</description></item><item><title>贝叶斯统计之概述</title><link>https://hongtaoh.com/cn/2020/01/18/bayesian-stats-chapter2/</link><pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate><guid>https://hongtaoh.com/cn/2020/01/18/bayesian-stats-chapter2/</guid><description>前言：这篇文章是我上约翰.克鲁斯克老师 (John Kruschke ) 的《贝叶斯数据分析》 (P533: Bayesian Data Analysis ) 这门课时，在课下看他的教材 Doing Bayesian Data Analysis (Second Edition) 所做的笔记。
贝叶斯的本质是对于可能性1(credibility) 的重置 (reallocation)。比如，早上刚出家门，你看到门前地面是湿的。这个有很多种可能的原因：下雨了、小孩子撒尿了、环卫工人刚打扫了、物业洒水车刚洒了水、有人不小心把水杯里的水洒出来了、外星人来过自己的家然后离开的时候不小心留下的痕迹&amp;hellip;这些所有的可能，在我们获得进一步数据之前，都有着自己的先验概率 (prior probability)。这些先验概率，是我们主观上认为的可能性，比如根据我们往常的生活经验，下雨的可能性要比外星人到访高的多。虽然高得多，我们获得的信息毕竟是有限的，因为我们只是观察了门前的一小块地。因此，这些可能性都是存在的。
然后，我们接着在路上走，发现不仅整条路都是湿的，而且房屋顶都是湿的，那么我们对概率进行重置：下雨的可能性进步一提升，而之前有可能的洒水车洒过水的可能性下降，因为洒水车不可能把水洒到房顶上。
Kruschke 老师的教材中的图2.1 对这一过程进行了进一步阐释。这张图展示了从一开始认为 A, B, C, D 都有可能是罪犯，而且可能性相同，然后一步一步找到新的线索，最后认定 D 是罪犯的过程。
图2.1, 来源: Kruschke 老师的教材 p.17 贝叶斯统计中很关键的一步是找到可能的解释情况。比如看到门前的路面是湿的，我们已经想到了很多可能的情况，但是不可能把这些情况穷尽。比如，也有可能是一个人在正好经过你的家门的时候，接到一个电话，然后她就哭了，于是门前的路面就湿了，等等。我们也不用试着去穷尽可能情况，只需要找到我们感兴趣的几个情况就好。通过分析，我们可以看到这几个情况是否很好地解释了我们观察到的数据，如果没有，那么我们再去找别的可能的情况。这一过程叫做 posterior predictive check.
很多的时候，一种可能性由一个数学公式来代表，然后看哪个公式可以更好得描述观察到的数据（具体来说，是看是否契合数据的趋势 (trends)以及分散度 (spread) 。从图2.4 可以看到， 两个数学公式中，明显第一个公式更好地描述了观察到的数据。需要注意的是，数学公式和观察到的数据之间并不存在因果关系。
图2.4, 来源: Kruschke 老师的教材 p.23 Kruschke 老师举了一个身高－体重的例子。图2.5 左边的图中，小圆圈代表了具体的每个人的身高－体重数据。
图2.5, 来源: Kruschke 老师的教材 p.</description></item></channel></rss>
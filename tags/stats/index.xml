<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>stats on 郝鸿涛：Hongtao Hao</title><link>https://hongtaoh.com/tags/stats/</link><description>Recent content in stats on 郝鸿涛：Hongtao Hao</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 22 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://hongtaoh.com/tags/stats/index.xml" rel="self" type="application/rss+xml"/><item><title>Re-Understanding Pearson Correlation Coefficient</title><link>https://hongtaoh.com/en/2022/08/22/corr2/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://hongtaoh.com/en/2022/08/22/corr2/</guid><description>Everyone seems to know correlation but they do not. At least I didn&amp;rsquo;t.
To understand correlation, we need to first understand covariance. As the name suggests, covariance measures the extent to which two variables covary, i.e., vary together. But the questions are: what does &amp;ldquo;vary&amp;rdquo; mean, and how do we quantify &amp;ldquo;together&amp;rdquo;?
For example, I give you these two vectors:
import matplotlib.pyplot as plt a = [2, 4, 6, 8] b = [1, 2, 3, 8] Can you tell me the extent to which they &amp;ldquo;vary together&amp;rdquo;?</description></item><item><title>Re-Understanding Z Score</title><link>https://hongtaoh.com/en/2022/08/20/zscore/</link><pubDate>Sat, 20 Aug 2022 00:00:00 +0000</pubDate><guid>https://hongtaoh.com/en/2022/08/20/zscore/</guid><description>Ask anyone who has attended Stats101 and s/he will tell you that they understand Z-score. But, really? Could you answer the following questions without thinking?
What is the mean of z-scores? What is the standard deviation of z-scores? What is the sum of squared z-scores? Is the z-score distribution the same as the original distribution of sample values? What do z-scores above 0 mean? If you cannot answer them without thinking, then you don&amp;rsquo;t really understand z-scores.</description></item><item><title>Cutoff Values for Confirmatory Factor Analysis</title><link>https://hongtaoh.com/en/2022/07/14/cfa/</link><pubDate>Thu, 14 Jul 2022 19:59:06 -0500</pubDate><guid>https://hongtaoh.com/en/2022/07/14/cfa/</guid><description>Below are some of my notes on the key cutoff values for confirmatory factor analysis (CFA). They are based on this PDF by Professor Lesa Hoffman .
Smaller chi square, and larger $p$-value are better (we don&amp;rsquo;t want the p-value to be smaller than .05)
SRMR (smaller is better; &amp;lt;.08 -&amp;gt; good fit)
RMSEA, smaller is better. &amp;lt;.05 or &amp;lt;.06 -&amp;gt; good; .05~.08 -&amp;gt; acceptable; .</description></item><item><title>Understanding Correlations</title><link>https://hongtaoh.com/en/2022/07/14/corr/</link><pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate><guid>https://hongtaoh.com/en/2022/07/14/corr/</guid><description>This post is based on the book of Think Stats 2e by Allen B. Downey.
Load package and data:
library(ggplot2) df &amp;lt;- read.csv(&amp;quot;../../../../static/files/selfie-data.csv&amp;quot;) Plotting Scatter plot
ggplot(df, aes(x=height, y=weight)) + geom_point() + xlab(&amp;quot;Height (in cm)&amp;quot;) + ylab(&amp;quot;Weight (in kg)&amp;quot;) The problem is that people might round their weight and height off. For example, 164.5 cm will be rounded as 165 cm. To reverse the rounding effect, we can add some random noise.</description></item></channel></rss>